{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b2bee6",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ“˜ RAG Deep-Dive â€” Conceptual Guide (Aligned with Your TOC)\n",
    "\n",
    "This notebook gives you **high-level explanations** of the main RAG concepts,  \n",
    "aligned with your Table of Contents and Roadmap.\n",
    "\n",
    "> ðŸŽ¯ Goal: build **strong intuition** for each part of a RAG system.  \n",
    "> ðŸ’¡ Focus: concepts, examples, best practices â€” **no heavy code**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb78bc3",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# ðŸŸ¦ Phase 1 â€” Foundations (TOC Ch. 1â€“3)\n",
    "\n",
    "## 1. What is RAG (Retrieval-Augmented Generation)?\n",
    "\n",
    "RAG is a pattern where an LLM **does not answer purely from its internal weights**.  \n",
    "Instead, it first **retrieves external information** (documents, snippets, records)  \n",
    "and then uses that context to generate an answer.\n",
    "\n",
    "- Without RAG: *Model guesses from what it \"remembers\".*\n",
    "- With RAG: *Model reads from your data, then answers.*\n",
    "\n",
    "You can think of it as:\n",
    "> **â€œSearch first, then generate.â€**  \n",
    "\n",
    "This makes RAG:\n",
    "- More **up to date** (you can index new data),\n",
    "- More **controllable** (you decide which data it sees),\n",
    "- More **grounded** (answers can cite sources).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Why RAG vs Fine-Tuning?\n",
    "\n",
    "**Fine-tuning** changes the model weights to adapt it to a task or data distribution.  \n",
    "**RAG** keeps the model *frozen* but feeds it relevant external knowledge at query time.\n",
    "\n",
    "**RAG advantages:**\n",
    "- No need to retrain when data changes â€” just re-index.\n",
    "- Easier to control access (e.g., per-user documents).\n",
    "- Typically cheaper and safer for most knowledge-heavy apps.\n",
    "\n",
    "**Fine-tuning advantages:**\n",
    "- Good for *skills* (formatting, style, domain tone) and niche tasks.\n",
    "- Can compress patterns into the model.\n",
    "\n",
    "Often, real systems use: **RAG + small fine-tuning** (for style or tools).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Where RAG Lives in the LLM Stack\n",
    "\n",
    "Very roughly, the stack looks like:\n",
    "\n",
    "1. **Raw data** (PDFs, DBs, APIs)  \n",
    "2. **Indexing** (chunking + embeddings + vector DB)  \n",
    "3. **Orchestration** (retriever, prompts, tools, policies)  \n",
    "4. **LLM inference** (generation)  \n",
    "5. **Evaluation & monitoring**\n",
    "\n",
    "RAG is **not the model itself** â€” it is the **system around the model**:  \n",
    "- How we prepare and store data  \n",
    "- How we retrieve and inject it  \n",
    "- How we control and evaluate responses\n",
    "\n",
    "Understanding this prevents you from thinking â€œRAG = one function callâ€.  \n",
    "It is an **architecture pattern**, not a single tool.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Pre-RAG Landscape (TF-IDF, BM25, Classic IR)\n",
    "\n",
    "Before embeddings and RAG, search used mostly **lexical** methods:\n",
    "- TF-IDF: score based on how frequently words appear.\n",
    "- BM25: improved variant of TF-IDF, still word-based.\n",
    "\n",
    "These methods work well for:\n",
    "- Exact word matches,\n",
    "- Short documents,\n",
    "- Keyword-style queries.\n",
    "\n",
    "They struggle with:\n",
    "- Synonyms (â€œdoctorâ€ vs â€œphysicianâ€),\n",
    "- Semantic meaning (â€œheart attackâ€ vs â€œmyocardial infarctionâ€),\n",
    "- Long, fuzzy questions.\n",
    "\n",
    "Embeddings + RAG **extend** this tradition with **semantic search**, not replace it entirely â€”  \n",
    "in modern systems, **hybrid** (BM25 + vectors) is common.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Minimal LLM Background for RAG\n",
    "\n",
    "You donâ€™t need full transformer math, but you should know:\n",
    "\n",
    "- **Tokens**: Models read tokens, not raw characters.\n",
    "- **Context window**: Hard limit to how much text you can pass at once.\n",
    "- **Prompt structure**: System message, user message, tool calls, etc.\n",
    "- **Hallucinations**: Model â€œinventingâ€ plausible but false answers.\n",
    "\n",
    "RAG helps with hallucinations by showing the model the right context,  \n",
    "but does **not eliminate them** â€” you still need good prompts and evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597d108f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# ðŸŸ© Phase 2 â€” Core RAG Pipeline (TOC Ch. 4â€“13)\n",
    "\n",
    "## 6. Data Preparation (Ingestion)\n",
    "\n",
    "RAG starts with **raw data**:\n",
    "- PDFs, HTML pages, markdown docs, slides\n",
    "- Databases, CSVs, APIs\n",
    "- Internal knowledge bases, tickets, logs\n",
    "\n",
    "Typical steps:\n",
    "1. **Extract** text (PDF â†’ text, HTML â†’ text, etc.).  \n",
    "2. **Normalize** (strip boilerplate, remove headers/footers).  \n",
    "3. **Enrich** with **metadata**: title, section, date, author, tags, source URL.  \n",
    "\n",
    "Why metadata matters:\n",
    "- Lets you filter: e.g., only â€œpoliciesâ€, or only â€œlast 6 monthsâ€.\n",
    "- Improves re-ranking and answer citations.\n",
    "\n",
    "> Good ingestion is like cleaning data before ML â€” if you skip it, everything downstream suffers.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Chunking Strategies\n",
    "\n",
    "LLMs cannot read entire books in one prompt, so we **split documents into chunks**.\n",
    "\n",
    "### 7.1 Why Chunking is Harder Than â€œEvery 500 Tokensâ€\n",
    "\n",
    "Naive chunking (e.g., every 500 characters) can split mid-sentence or mid-section.  \n",
    "This can:\n",
    "- Break context,\n",
    "- Make retrieval less useful,\n",
    "- Increase noise.\n",
    "\n",
    "Better strategies aim to preserve **semantic units**:\n",
    "- Paragraphs,\n",
    "- Sections,\n",
    "- Headings,\n",
    "- Code blocks (for code RAG),\n",
    "- Clauses in legal/medical text.\n",
    "\n",
    "### 7.2 Common Strategies\n",
    "\n",
    "- **Fixed-size + overlap**:  \n",
    "  - Example: 512 tokens with 128 overlap.  \n",
    "  - Simple, robust, easy to implement.\n",
    "\n",
    "- **Recursive / structural chunking**:  \n",
    "  - Use headings (H1/H2/H3) and paragraph boundaries first.  \n",
    "  - Then further split long sections.\n",
    "\n",
    "- **Semantic chunking**:  \n",
    "  - Use models to group sentences that belong together conceptually.  \n",
    "  - More complex, but can improve retrieval quality.\n",
    "\n",
    "Key idea:\n",
    "> You want chunks that are **small enough** to be precise, but **large enough** to contain a meaningful answer.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Embeddings â€” Turning Text into Vectors\n",
    "\n",
    "Embeddings are functions that map text â†’ vector of numbers (e.g., 768 dimensions).  \n",
    "Similar texts have vectors that are **close** in this space.\n",
    "\n",
    "Types to know:\n",
    "\n",
    "- **Dense embeddings**: e.g., OpenAI, sentence transformers.  \n",
    "- **Sparse embeddings**: e.g., BM25 or newer sparse models, closer to keyword search.  \n",
    "- **Hybrid**: combine both to get best of lexical + semantic search.\n",
    "\n",
    "Important design decisions:\n",
    "- Which embedding model (domain-specific models often help).  \n",
    "- How long the text per embedding (chunk size).  \n",
    "- Whether to use multi-vector embeddings (separate vectors per field).\n",
    "\n",
    "You usually **pre-compute embeddings** for all chunks and store them in a vector DB.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Vector Databases\n",
    "\n",
    "A vector DB stores:\n",
    "- The embedding vectors, and\n",
    "- The original text chunks + metadata.\n",
    "\n",
    "On query:\n",
    "1. Embed the **query**.  \n",
    "2. Perform **k-nearest neighbors** search (k-NN or ANN).  \n",
    "3. Return top-k chunks.\n",
    "\n",
    "Key concepts:\n",
    "- **Index type** (HNSW, IVF, PQ) affects speed vs accuracy.\n",
    "- **Filtering** (by metadata) lets you restrict to relevant subsets.\n",
    "- **Batching** helps when indexing lots of documents.\n",
    "\n",
    "Think of vector DB as:\n",
    "> â€œA search engine that understands meaning, not just words.â€\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Retrieval & Reranking\n",
    "\n",
    "### 10.1 Retrieval\n",
    "\n",
    "Base retrieval:  \n",
    "> Given query vector, find closest chunk vectors.\n",
    "\n",
    "You control:\n",
    "- **k** (how many to retrieve),\n",
    "- Optional **filters** (e.g., only docs from 2024).\n",
    "\n",
    "### 10.2 Reranking\n",
    "\n",
    "Top-k raw retrieval often returns noisy or mediocre results.  \n",
    "**Rerankers** re-score the candidate chunks using:\n",
    "\n",
    "- A **cross-encoder** (small model that reads query + chunk together), or  \n",
    "- The **LLM itself** (LLM-as-reranker).\n",
    "\n",
    "This step improves:\n",
    "- **Relevance** (top 3â€“5 chunks really match the question),\n",
    "- **Faithfulness** (better grounding).\n",
    "\n",
    "Trade-off: more accurate, but more latency and cost.\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Query Optimization: Expansion & Rewriting\n",
    "\n",
    "Real user queries can be:\n",
    "- Ambiguous,\n",
    "- Too short,\n",
    "- Using odd phrasing.\n",
    "\n",
    "We can improve retrieval by first **rewriting** or **expanding** them.\n",
    "\n",
    "Examples:\n",
    "- **Multi-query RAG**: generate several paraphrased queries and retrieve for each.  \n",
    "- **HyDE**: generate a hypothetical answer/document, embed that, and retrieve similar chunks.  \n",
    "- **Self-querying**: model writes a structured query (filters, keywords) for the vector DB.\n",
    "\n",
    "All of this happens **before retrieval**, to improve the candidate pool.\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Prompt Engineering in RAG\n",
    "\n",
    "Once we have retrieved chunks, we need to **feed them to the LLM**:\n",
    "\n",
    "Key ideas:\n",
    "- Use a **template**: system instructions + user query + context.  \n",
    "- Clearly mark context vs question so the model knows what is â€œevidenceâ€.  \n",
    "- Ask for **citations** (e.g., â€œcite sources in parenthesesâ€).  \n",
    "- Add **style constraints**: level of detail, target audience, format (JSON, markdown, etc.).\n",
    "\n",
    "Example conceptual structure:\n",
    "\n",
    "> System: â€œYou are a helpful assistant who must only use the provided context.â€  \n",
    "> Context: â€œ(list of retrieved chunks)â€  \n",
    "> User: â€œQuestion: â€¦â€\n",
    "\n",
    "Good prompts:\n",
    "- Emphasize **grounding** (â€œIf the answer is not in the context, say you donâ€™t know.â€),\n",
    "- Control hallucinations with explicit rules.\n",
    "\n",
    "---\n",
    "\n",
    "## 13. Answer Generation & Advanced Strategies\n",
    "\n",
    "Basic RAG:\n",
    "1. Retrieve chunks.  \n",
    "2. Build prompt.  \n",
    "3. Generate answer once.\n",
    "\n",
    "Advanced RAG strategies:\n",
    "- **RAG-Fusion**: combine rankings from multiple queries or sources.  \n",
    "- **LLM-as-a-judge**: multiple candidate answers â†’ LLM picks the best.  \n",
    "- **Self-reflection**: model critiques its own answer and revises it using the context again.  \n",
    "- **ReAct + RAG**: model alternates between reasoning and tool calls (retrieval as a tool).\n",
    "\n",
    "All of these try to:\n",
    "- Reduce hallucinations,\n",
    "- Improve relevance,\n",
    "- Make the system more robust to tricky queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfb5489",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# ðŸŸ§ Phase 3 â€” RAG Types & Variants (TOC Ch. 14â€“17)\n",
    "\n",
    "## 14. Basic vs Advanced RAG\n",
    "\n",
    "**Basic RAG**:\n",
    "- Single retrieval step,\n",
    "- Single prompt with fixed template,\n",
    "- Single LLM call.\n",
    "\n",
    "**Advanced RAG**:\n",
    "- Multiple retrieval rounds,\n",
    "- Adaptive prompts,\n",
    "- Multiple models / tools interacting,\n",
    "- Routing based on query type.\n",
    "\n",
    "Think of basic RAG as a **single function**, advanced RAG as a **small system**.\n",
    "\n",
    "---\n",
    "\n",
    "## 15. Multi-Query, Fusion, and HyDE\n",
    "\n",
    "### Multi-Query RAG\n",
    "\n",
    "- Idea: generate multiple variants of the user query.\n",
    "- Retrieve for each variant.\n",
    "- Merge results (e.g., union, rerank by score).\n",
    "\n",
    "Benefit: covers different **wording** and **angles** of the question.\n",
    "\n",
    "### RAG-Fusion\n",
    "\n",
    "- Inspired by search engines that combine multiple result lists.  \n",
    "- Uses techniques like **Reciprocal Rank Fusion** to combine multiple rankings.\n",
    "\n",
    "Benefit: more robust retrieval, less sensitivity to one bad query.\n",
    "\n",
    "### HyDE (Hypothetical Document Embeddings)\n",
    "\n",
    "- Generate a â€œplausible answerâ€ or â€œhypothetical documentâ€ for the query.  \n",
    "- Embed that hypothetical document and retrieve similar real documents.\n",
    "\n",
    "Benefit: improves retrieval when there is little lexical overlap between question and docs  \n",
    "(e.g., different vocabularies in user vs documents).\n",
    "\n",
    "---\n",
    "\n",
    "## 16. GraphRAG & Knowledge Graph Integration\n",
    "\n",
    "In many domains, knowledge is **structured**:\n",
    "- Entities (people, drugs, companies, APIs),\n",
    "- Relationships (treats, depends on, born in, uses).\n",
    "\n",
    "**GraphRAG** uses a graph (knowledge graph or extracted graph) to:\n",
    "\n",
    "- Expand queries along related nodes (e.g., related concepts),\n",
    "- Rank documents using graph signals (centrality, communities),\n",
    "- Provide structured reasoning paths (like chains of entities).\n",
    "\n",
    "You can think of it as:\n",
    "> â€œRAG where retrieval is guided by relationships, not just vector similarity.â€\n",
    "\n",
    "---\n",
    "\n",
    "## 17. Multi-Modal RAG\n",
    "\n",
    "RAG doesnâ€™t have to be text-only.\n",
    "\n",
    "Examples:\n",
    "- Retrieve images related to a question,\n",
    "- Retrieve audio transcripts,\n",
    "- Retrieve charts or structured tables.\n",
    "\n",
    "Typical pattern:\n",
    "- Use modality-specific embeddings (image embeddings, audio embeddings),\n",
    "- Use a unified index or multiple coordinated indexes,\n",
    "- Convert retrieved non-text artifacts into text or features LLM can handle.\n",
    "\n",
    "---\n",
    "\n",
    "## 18. Agentic RAG\n",
    "\n",
    "An **agent** is an LLM that can:\n",
    "- Decide what tools to call,\n",
    "- Make multi-step plans,\n",
    "- Reflect and refine its outputs.\n",
    "\n",
    "RAG becomes one of the agentâ€™s **tools**, such as:\n",
    "- `search_docs(query)`,\n",
    "- `get_user_profile(user_id)`,\n",
    "- `call_api(endpoint, params)`.\n",
    "\n",
    "Agentic RAG patterns:\n",
    "- **Plannerâ€“executor**: one agent plans, another executes retrieval and answering.  \n",
    "- **Multi-agent teams**: e.g., one agent specializing in retrieval, another in explanation.  \n",
    "\n",
    "Benefit:\n",
    "- More flexible, can handle complex tasks with multiple steps and decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea1a6c3",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# ðŸŸ¥ Phase 4 â€” Evaluation & Observability (TOC Ch. 18â€“20)\n",
    "\n",
    "## 19. Why RAG Evaluation is Tricky\n",
    "\n",
    "Traditional ML:\n",
    "- You have labels, you compute accuracy, precision, etc.\n",
    "\n",
    "RAG:\n",
    "- Thereâ€™s retrieval and generation,\n",
    "- There may be many acceptable answers,\n",
    "- Ground truth documents may be incomplete.\n",
    "\n",
    "You need to evaluate at **three levels**:\n",
    "1. Retrieval quality (are we fetching good chunks?).  \n",
    "2. Answer quality (is the answer correct, clear, useful?).  \n",
    "3. Faithfulness (does the answer stick to the context?).\n",
    "\n",
    "---\n",
    "\n",
    "## 20. Classical Metrics: Precision, Recall, MRR\n",
    "\n",
    "For retrieval, you can still use:\n",
    "- **Precision@k**: among top-k retrieved docs, how many are relevant?  \n",
    "- **Recall@k**: among all relevant docs, how many did we retrieve?  \n",
    "- **MRR** (Mean Reciprocal Rank): how high is the first relevant document in the list?\n",
    "\n",
    "These require:\n",
    "- A set of queries,\n",
    "- For each, a set of â€œrelevantâ€ documents.\n",
    "\n",
    "You can build small evaluation sets manually, especially for critical use cases.\n",
    "\n",
    "---\n",
    "\n",
    "## 21. RAGAS, TruLens, LLM-as-a-Judge\n",
    "\n",
    "Because manually labeling is expensive, people use **LLM-based evaluation**.\n",
    "\n",
    "### RAGAS (Retrieval Augmented Generation Assessment)\n",
    "\n",
    "RAGAS defines metrics like:\n",
    "- **Context relevance**: are the retrieved chunks related to the question?  \n",
    "- **Answer relevance / correctness**: does the answer address the question?  \n",
    "- **Faithfulness / groundedness**: is the answer supported by the context?  \n",
    "\n",
    "Implementation typically uses an LLM to **score** these attributes.\n",
    "\n",
    "### LLM-as-a-Judge\n",
    "\n",
    "Instead of a fixed metric, you ask an LLM to rate answers:\n",
    "- Compare candidate answers: â€œWhich is better and why?â€  \n",
    "- Score 1â€“10 based on correctness, clarity, grounding, etc.\n",
    "\n",
    "Pitfall:\n",
    "- The judge model can be biased or inconsistent,\n",
    "- But it scales better than only human evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## 22. Observability: Logging, Tracing, Diagnostics\n",
    "\n",
    "Even without fancy metrics, basic **observability** will dramatically help:\n",
    "- Log each query,\n",
    "- Log retrieved chunks (IDs + snippets + scores),\n",
    "- Log final answer and any errors.\n",
    "\n",
    "You can then:\n",
    "- Inspect bad answers,\n",
    "- See if retrieval was the problem (bad chunks) or generation (hallucination).\n",
    "\n",
    "**Tracing** (with spans) lets you see:\n",
    "- Time spent in each step (embedding, retrieval, LLM call),\n",
    "- Where time or errors spike,\n",
    "- Where you need to optimize or add safeguards.\n",
    "\n",
    "> A RAG system without logs is a black box.  \n",
    "> A RAG system with good logs becomes debuggable and tunable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0ddc91",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# ðŸŸ¨ Phase 5 â€” Performance Engineering (TOC Ch. 21â€“23)\n",
    "\n",
    "## 23. Latency Sources in RAG\n",
    "\n",
    "Where time goes:\n",
    "1. Embedding the query,\n",
    "2. Querying the vector DB,\n",
    "3. Calling the LLM (often the biggest chunk),\n",
    "4. Optional reranking / multiple rounds.\n",
    "\n",
    "Levers to reduce latency:\n",
    "- Cache embeddings (donâ€™t re-embed same queries),\n",
    "- Reduce k (top-k) if possible,\n",
    "- Use faster indexes / hardware,\n",
    "- Use smaller or more efficient LLMs when appropriate,\n",
    "- Avoid unnecessary multi-step chains for simple questions.\n",
    "\n",
    "---\n",
    "\n",
    "## 24. Cost Optimization\n",
    "\n",
    "Cost usually dominated by:\n",
    "- Embedding calls (for large corpora),\n",
    "- LLM generation (for long answers, big models).\n",
    "\n",
    "Strategies:\n",
    "- Embed once, reuse many times (batch ingestion).  \n",
    "- Use shorter contexts (only most relevant chunks).  \n",
    "- Route between **big** and **small** models based on query importance.  \n",
    "- Cache answers for identical or similar queries (answer cache).\n",
    "\n",
    "> Rule of thumb: â€œDonâ€™t pay GPT-4 prices for FAQ-level questions.â€\n",
    "\n",
    "---\n",
    "\n",
    "## 25. Scalability\n",
    "\n",
    "As your dataset grows:\n",
    "- Vector indexes grow,\n",
    "- Retrieval may slow,\n",
    "- Memory and disk usage increase.\n",
    "\n",
    "Scalability tools:\n",
    "- **Sharding**: split index into multiple parts across machines.  \n",
    "- **Replication**: duplicate indexes for high availability & read scaling.  \n",
    "- **Tiered storage**: keep freshest or most-used data on fast storage.\n",
    "\n",
    "At application level:\n",
    "- Use stateless RAG services behind a load balancer,\n",
    "- Decouple indexing (offline jobs) from serving (online queries).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9966b5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# ðŸŸª Phase 6 â€” System Architecture & Enterprise RAG (TOC Ch. 24â€“26)\n",
    "\n",
    "## 26. RAG Architecture Patterns\n",
    "\n",
    "Common patterns:\n",
    "\n",
    "- **Local/dev RAG**:  \n",
    "  - All on one machine, small vector store, simple stack.\n",
    "\n",
    "- **Cloud RAG**:  \n",
    "  - Managed vector DB (Pinecone, Weaviate, etc.),  \n",
    "  - LLM API (OpenAI, etc.),  \n",
    "  - Backend service orchestrating everything.\n",
    "\n",
    "- **Hybrid RAG**:  \n",
    "  - Some data/indexes on-prem (sensitive),  \n",
    "  - Some in cloud (public docs),  \n",
    "  - Orchestrator routes queries accordingly.\n",
    "\n",
    "Each pattern balances:\n",
    "- Data residency / privacy,\n",
    "- Latency,\n",
    "- Cost,\n",
    "- Operational complexity.\n",
    "\n",
    "---\n",
    "\n",
    "## 27. Knowledge Graph + RAG (GraphRAG)\n",
    "\n",
    "Key idea:\n",
    "- Use **structured relationships** between entities to improve retrieval and reasoning.\n",
    "\n",
    "Benefits:\n",
    "- Can follow paths like: A â†’ B â†’ C in the graph,\n",
    "- Can detect clusters / communities,\n",
    "- Can prioritize central or authoritative entities.\n",
    "\n",
    "GraphRAG often involves:\n",
    "- Extracting entities and relations from text,\n",
    "- Building a graph DB (e.g., Neo4j),\n",
    "- Combining graph traversal with vector retrieval,\n",
    "- Providing the LLM with both textual and structural context.\n",
    "\n",
    "---\n",
    "\n",
    "## 28. Enterprise Concerns: Access Control, Governance, Compliance\n",
    "\n",
    "When RAG is used in a company:\n",
    "- Not all users should see all documents,\n",
    "- Some data may be private, secret, or regulated.\n",
    "\n",
    "Key requirements:\n",
    "- **Row-level / document-level access control**:  \n",
    "  - Filter results by user permissions before returning them.\n",
    "\n",
    "- **Audit logs**:  \n",
    "  - Who queried what, what data was used, what answer produced.\n",
    "\n",
    "- **PII & compliance**:  \n",
    "  - Some content may need masking or special handling (GDPR, HIPAA, etc.).\n",
    "\n",
    "Designing enterprise RAG means thinking beyond â€œdoes it work?â€ to â€œis it safe, fair, compliant?â€.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2538d3e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# ðŸŸ« Phase 7 â€” Frameworks & Deployment (TOC Ch. 27â€“29)\n",
    "\n",
    "## 29. RAG Frameworks\n",
    "\n",
    "Popular libraries:\n",
    "- **LangChain / LangChain.js**  \n",
    "- **LlamaIndex**  \n",
    "- **Haystack**  \n",
    "- **DSPy**  \n",
    "- **Semantic Kernel**  \n",
    "\n",
    "They provide:\n",
    "- Pre-built components (retrievers, chains, agents),\n",
    "- Integrations with LLMs and vector DBs,\n",
    "- Common patterns (RAG chains, tool calling, evaluators).\n",
    "\n",
    "Trade-offs:\n",
    "- Speed of development vs flexibility,\n",
    "- Abstraction overhead vs control,\n",
    "- Ecosystem and community support.\n",
    "\n",
    "A good path:\n",
    "> Implement one RAG pipeline **by hand** (raw code), then adopt a framework once concepts are solid.\n",
    "\n",
    "---\n",
    "\n",
    "## 30. Vector DB Internals (Deep Dive)\n",
    "\n",
    "At a higher level, you should know:\n",
    "- How indexes are created,\n",
    "- How updates and deletions work,\n",
    "- How filters and metadata-based search operate,\n",
    "- How replication and backups are handled.\n",
    "\n",
    "This helps you:\n",
    "- Debug performance issues,\n",
    "- Choose configs tuned to your workload,\n",
    "- Make better architectural decisions.\n",
    "\n",
    "---\n",
    "\n",
    "## 31. Deployment: From Notebook to Service\n",
    "\n",
    "Going from Colab/VS Code to real app involves:\n",
    "\n",
    "1. **Wrap RAG in an API**  \n",
    "   - e.g., FastAPI, Express, NestJS, etc.\n",
    "\n",
    "2. **Containerize with Docker**  \n",
    "   - So it runs consistently across environments.\n",
    "\n",
    "3. **Deploy** to:  \n",
    "   - Cloud VM,\n",
    "   - Serverless function,\n",
    "   - Kubernetes cluster.\n",
    "\n",
    "4. **Add CI/CD**  \n",
    "   - Automated tests,\n",
    "   - Linting,\n",
    "   - Deployment pipelines.\n",
    "\n",
    "The RAG logic doesnâ€™t change much â€” but you add **infrastructure** around it so others can safely use it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb469c7",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# ðŸŸ¦ Phase 8 â€” Frontier Research & RAG 2.0 (TOC Ch. 30â€“32)\n",
    "\n",
    "## 32. RAG 2.0 and Beyond\n",
    "\n",
    "Recent research and industry trends push RAG further:\n",
    "\n",
    "- **Self-correcting RAG**:  \n",
    "  - Model critiques its own answer using the context and revises if needed.\n",
    "\n",
    "- **Dynamic / adaptive retrieval**:  \n",
    "  - Model decides **how much** to retrieve, or whether to retrieve at all.\n",
    "\n",
    "- **Memory-augmented RAG**:  \n",
    "  - Long-term memory of interactions and documents, not just one-off queries.\n",
    "\n",
    "---\n",
    "\n",
    "## 33. Model-Assisted Retrieval & Indexing\n",
    "\n",
    "Instead of static pipelines, models themselves help:\n",
    "\n",
    "- Suggest what to index and how,\n",
    "- Decide which fields matter most,\n",
    "- Learn better embeddings for a domain,\n",
    "- Optimize retrieval over time from feedback.\n",
    "\n",
    "This blurs the line between â€œretrieverâ€ and â€œmodelâ€:  \n",
    "> Retrieval is not just a static index â€” it becomes a **learning component** in the system.\n",
    "\n",
    "---\n",
    "\n",
    "## 34. Future Directions: Agents, Multimodal, and Infrastructure\n",
    "\n",
    "Likely trends:\n",
    "\n",
    "- RAG as the **default pattern** in enterprise LLM systems, not the exception.  \n",
    "- RAG deeply integrated with **multi-agent systems**, where retrieval is just one capability among many.  \n",
    "- **Multimodal RAG** that reasons over text, code, images, audio, logs, telemetry.  \n",
    "- RAG becoming **infrastructure**: standardized interfaces, shared services, policies.\n",
    "\n",
    "Your advantage:\n",
    "> Understanding the foundations and the current frontier means you can adapt as tools and models evolve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58722ffb",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## âœ… How to Use This Deep-Dive Notebook\n",
    "\n",
    "- Read it **phase by phase**, ideally in parallel with:\n",
    "  - The **Roadmap notebook** (for checklists + planning), and\n",
    "  - The **Hands-On notebook** (for actual implementation).\n",
    "\n",
    "- Add your own notes under each section:\n",
    "  - Links to resources,\n",
    "  - Examples from your projects,\n",
    "  - Diagrams,\n",
    "  - Questions to clarify later.\n",
    "\n",
    "This notebook is your **conceptual backbone** â€”  \n",
    "once you feel comfortable with each section here, youâ€™ll be in a strong position to:\n",
    "\n",
    "- Design RAG systems,\n",
    "- Debug and improve them,\n",
    "- Explain them in interviews or presentations,\n",
    "- And specialize into domains like finance, medicine, legal, or code RAG.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
