{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d682065e",
   "metadata": {},
   "source": [
    "\n",
    "# üêç RAG Hands-On (Python) ‚Äî Company Files Q&A Chatbot\n",
    "\n",
    "**Goal:** Build a **minimal but real** RAG pipeline in Python for Q&A over company files  \n",
    "(**PDF, DOCX, CSV**) with:\n",
    "\n",
    "- Multiple **LLM/chat models** (switchable),\n",
    "- Multiple **embedding models** (cloud + local),\n",
    "- Multiple **vector stores** (FAISS + Chroma, all offline-capable),\n",
    "- **Chunking** strategies,\n",
    "- **Conversation memory** (simple but effective),\n",
    "- Clean structure and comments so this becomes your **‚ÄúEntrance to the RAG Universe‚Äù**.\n",
    "\n",
    "> ‚ö†Ô∏è No Tavily, no web search, no UI ‚Äî just a clean backend-style pipeline you can test in a notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b855824",
   "metadata": {},
   "source": [
    "\n",
    "## 0. High-Level Architecture\n",
    "\n",
    "1. **Ingest**: Load PDF / DOCX / CSV from a `/data` directory.  \n",
    "2. **Chunk**: Split into overlapping text chunks.  \n",
    "3. **Embed**: Turn chunks into vectors using one of several embedding models.  \n",
    "4. **Index**: Store embeddings in a vector store (FAISS or Chroma).  \n",
    "5. **Retrieve**: Given a user question, pull top-k relevant chunks.  \n",
    "6. **RAG Generate**: Combine question + chunks into a prompt and call an LLM.  \n",
    "7. **Memory**: Keep track of previous turns and inject them into the prompt.  \n",
    "\n",
    "You can think of it as two main phases:\n",
    "\n",
    "- **Offline / Preprocessing**: ingest + chunk + embed + index.  \n",
    "- **Online / Query-time**: retrieve + generate + memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06e37eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Install dependencies (run once per environment)\n",
    "# In Colab you can uncomment these lines.\n",
    "# !pip install -q langchain langchain-community langchain-openai chromadb faiss-cpu sentence-transformers\n",
    "# !pip install -q pypdf python-docx pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9a64c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Imports & basic config\n",
    "\n",
    "import os\n",
    "from typing import List, Literal, Dict, Any\n",
    "\n",
    "# LangChain core\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Vector stores\n",
    "from langchain_community.vectorstores import FAISS, Chroma\n",
    "\n",
    "# Embeddings\n",
    "from langchain_openai import OpenAIEmbeddings  # cloud\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings  # local\n",
    "\n",
    "# LLMs (chat models)\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# For loading files\n",
    "from pypdf import PdfReader\n",
    "from docx import Document as DocxDocument\n",
    "import pandas as pd\n",
    "\n",
    "# ---- API keys / env vars ----\n",
    "# Set these in your environment before running:\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "DATA_DIR = \"./data\"       # put your PDFs, DOCX, CSVs here\n",
    "CHROMA_DIR = \"./chroma_db\"  # for Chroma persistence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa8427f",
   "metadata": {},
   "source": [
    "\n",
    "## 3. File Loaders (PDF, DOCX, CSV)\n",
    "\n",
    "We keep this simple and transparent.\n",
    "\n",
    "- **PDF** ‚Üí `pypdf` (per page text).  \n",
    "- **DOCX** ‚Üí `python-docx` (paragraph join).  \n",
    "- **CSV** ‚Üí `pandas` (join columns per row).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa5d9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_pdf(path: str) -> List[Document]:\n",
    "    reader = PdfReader(path)\n",
    "    docs = []\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        text = page.extract_text() or \"\"\n",
    "        if text.strip():\n",
    "            docs.append(Document(\n",
    "                page_content=text,\n",
    "                metadata={\"source\": path, \"page\": i}\n",
    "            ))\n",
    "    return docs\n",
    "\n",
    "def load_docx(path: str) -> List[Document]:\n",
    "    d = DocxDocument(path)\n",
    "    paragraphs = [p.text for p in d.paragraphs if p.text.strip()]\n",
    "    text = \"\\n\".join(paragraphs)\n",
    "    return [Document(page_content=text, metadata={\"source\": path})]\n",
    "\n",
    "def load_csv(path: str, text_cols: List[str] = None) -> List[Document]:\n",
    "    df = pd.read_csv(path)\n",
    "    if text_cols is None:\n",
    "        # naive: use all columns\n",
    "        text_cols = list(df.columns)\n",
    "    docs = []\n",
    "    for idx, row in df.iterrows():\n",
    "        pieces = [f\"{col}: {row[col]}\" for col in text_cols if pd.notnull(row[col])]\n",
    "        text = \"\\n\".join(pieces)\n",
    "        if text.strip():\n",
    "            docs.append(Document(\n",
    "                page_content=text,\n",
    "                metadata={\"source\": path, \"row\": int(idx)}\n",
    "            ))\n",
    "    return docs\n",
    "\n",
    "def load_all_documents(data_dir: str = DATA_DIR) -> List[Document]:\n",
    "    all_docs: List[Document] = []\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for fname in files:\n",
    "            path = os.path.join(root, fname)\n",
    "            if fname.lower().endswith(\".pdf\"):\n",
    "                all_docs.extend(load_pdf(path))\n",
    "            elif fname.lower().endswith(\".docx\"):\n",
    "                all_docs.extend(load_docx(path))\n",
    "            elif fname.lower().endswith(\".csv\"):\n",
    "                all_docs.extend(load_csv(path))\n",
    "    return all_docs\n",
    "\n",
    "docs = load_all_documents()\n",
    "print(f\"Loaded {len(docs)} raw docs/chunks before splitting.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3398d3",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Chunking / Splitting Strategies\n",
    "\n",
    "We‚Äôll use `RecursiveCharacterTextSplitter` with:\n",
    "\n",
    "- **Chunk size**: 800‚Äì1200 characters (tweak based on your domain),\n",
    "- **Overlap**: 150‚Äì250 characters to preserve context across boundaries,\n",
    "- Optional splits on headings / newlines if desired.\n",
    "\n",
    "You can experiment with different configurations in one place.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed0576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunk_documents(\n",
    "    docs: List[Document],\n",
    "    chunk_size: int = 1000,\n",
    "    chunk_overlap: int = 200\n",
    ") -> List[Document]:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    )\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "chunked_docs = chunk_documents(docs)\n",
    "print(f\"After chunking: {len(chunked_docs)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c500140a",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Embedding Models (Multiple Options)\n",
    "\n",
    "We define an **embedding registry** so you can switch between:\n",
    "\n",
    "- `openai_small` ‚Üí small/cheap OpenAI embeddings (cloud).  \n",
    "- `openai_large` ‚Üí larger OpenAI embeddings (if you want).  \n",
    "- `mpnet_local` ‚Üí `all-mpnet-base-v2` (local SentenceTransformer).  \n",
    "\n",
    "You can easily add more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e0a8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EmbeddingName = Literal[\"openai_small\", \"openai_large\", \"mpnet_local\"]\n",
    "\n",
    "def get_embedding_model(name: EmbeddingName):\n",
    "    if name == \"openai_small\":\n",
    "        return OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    if name == \"openai_large\":\n",
    "        return OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    if name == \"mpnet_local\":\n",
    "        return SentenceTransformerEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    raise ValueError(f\"Unknown embedding model: {name}\")\n",
    "\n",
    "current_embedding_name: EmbeddingName = \"openai_small\"\n",
    "embeddings = get_embedding_model(current_embedding_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23d7caa",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Vector Stores (FAISS + Chroma)\n",
    "\n",
    "We‚Äôll support two popular, offline-friendly vector DBs:\n",
    "\n",
    "- **FAISS** (in-memory / file-based, fast ANN search):\n",
    "  - Great for local experiments and many production cases.\n",
    "- **Chroma** (persistent local DB):\n",
    "  - Nice developer experience, simple persistence.\n",
    "\n",
    "You can toggle which backend to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cbabff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "VectorStoreName = Literal[\"faiss\", \"chroma\"]\n",
    "\n",
    "def build_vectorstore(\n",
    "    chunks: List[Document],\n",
    "    store_name: VectorStoreName,\n",
    "    embeddings_model\n",
    "):\n",
    "    if store_name == \"faiss\":\n",
    "        vs = FAISS.from_documents(chunks, embeddings_model)\n",
    "        return vs\n",
    "    if store_name == \"chroma\":\n",
    "        vs = Chroma.from_documents(\n",
    "            chunks,\n",
    "            embeddings_model,\n",
    "            persist_directory=CHROMA_DIR\n",
    "        )\n",
    "        return vs\n",
    "    raise ValueError(f\"Unknown vector store: {store_name}\")\n",
    "\n",
    "\n",
    "current_vs_name: VectorStoreName = \"faiss\"\n",
    "vectorstore = build_vectorstore(chunked_docs, current_vs_name, embeddings)\n",
    "print(f\"Built vector store with backend = {current_vs_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ff9e61",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Chat Models (Multiple LLM Options)\n",
    "\n",
    "We define a small registry of chat models (you can pick whatever your account supports):\n",
    "\n",
    "- `gpt_4_small` ‚Üí e.g. `gpt-4o-mini`-class model (cheaper, fast).  \n",
    "- `gpt_4_full` ‚Üí e.g. `gpt-4.1`-class model (stronger).  \n",
    "- You can also plug in **local models** (LM Studio / Ollama) via a custom LLM class,  \n",
    "  but here we focus on OpenAI-style for simplicity.\n",
    "\n",
    "> Replace model names with whatever is available to you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec35b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ChatModelName = Literal[\"gpt_4_small\", \"gpt_4_full\"]\n",
    "\n",
    "def get_chat_model(name: ChatModelName):\n",
    "    if name == \"gpt_4_small\":\n",
    "        # adjust to your actual small/cheap model name\n",
    "        return ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    if name == \"gpt_4_full\":\n",
    "        # adjust to your actual strong model name\n",
    "        return ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "    raise ValueError(f\"Unknown chat model: {name}\")\n",
    "\n",
    "current_llm_name: ChatModelName = \"gpt_4_small\"\n",
    "llm = get_chat_model(current_llm_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e276c24f",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Simple Conversation Memory\n",
    "\n",
    "We‚Äôll implement a **minimal memory**:\n",
    "\n",
    "- Keep the last N turns of (user, assistant) pairs.\n",
    "- Inject them into the prompt before the current question.\n",
    "\n",
    "This is **manual but explicit** ‚Äî you see exactly what context the model sees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fedcd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import deque\n",
    "\n",
    "ConversationTurn = Dict[str, str]  # {\"user\": \"...\", \"assistant\": \"...\"}\n",
    "\n",
    "class SimpleConversationMemory:\n",
    "    def __init__(self, max_turns: int = 5):\n",
    "        self.max_turns = max_turns\n",
    "        self.history: deque[ConversationTurn] = deque(maxlen=max_turns)\n",
    "\n",
    "    def add_turn(self, user: str, assistant: str):\n",
    "        self.history.append({\"user\": user, \"assistant\": assistant})\n",
    "\n",
    "    def format_history(self) -> str:\n",
    "        lines = []\n",
    "        for turn in self.history:\n",
    "            lines.append(f\"User: {turn['user']}\")\n",
    "            lines.append(f\"Assistant: {turn['assistant']}\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "memory = SimpleConversationMemory(max_turns=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a56532b",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Retrieval + RAG Answer Function\n",
    "\n",
    "Now we wire everything together into a single function:\n",
    "\n",
    "1. Embed & retrieve top-k chunks from our vector store.  \n",
    "2. Format a prompt that includes:\n",
    "   - System instructions,\n",
    "   - Conversation history,\n",
    "   - Retrieved context,\n",
    "   - Latest user question.\n",
    "3. Call the chat model and return the answer.  \n",
    "4. Update memory with this turn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5713a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "def build_context_from_docs(docs: List[Document]) -> str:\n",
    "    parts = []\n",
    "    for i, d in enumerate(docs):\n",
    "        src = d.metadata.get(\"source\", \"unknown\")\n",
    "        ref = f\"[{i+1} | {os.path.basename(src)}]\"\n",
    "        parts.append(f\"{ref}\\n{d.page_content}\\n\")\n",
    "    return \"\\n---\\n\".join(parts)\n",
    "\n",
    "def rag_answer(\n",
    "    question: str,\n",
    "    k: int = 5,\n",
    "    system_prompt: str = (\n",
    "        \"You are a helpful assistant answering questions strictly based on the provided context. \"\n",
    "        \"If the answer is not in the context, say you don't know.\"\n",
    "    )\n",
    ") -> str:\n",
    "    # 1) Retrieve\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    context_text = build_context_from_docs(retrieved_docs)\n",
    "\n",
    "    # 2) Build messages (system + history + new question + context)\n",
    "    history_text = memory.format_history()\n",
    "    history_block = f\"\\n\\nConversation so far:\\n{history_text}\" if history_text else \"\"\n",
    "\n",
    "    full_system_prompt = (\n",
    "        system_prompt\n",
    "        + \"\\n\\nYou will be given context from company documents.\\n\"\n",
    "        + \"Use it to answer the question and cite references like [1], [2] where relevant.\"\n",
    "    )\n",
    "\n",
    "    final_user_content = (\n",
    "        f\"Context:\\n{context_text}\\n\\n\"\n",
    "        f\"{history_block}\\n\\n\"\n",
    "        f\"User question: {question}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=full_system_prompt),\n",
    "        HumanMessage(content=final_user_content),\n",
    "    ]\n",
    "\n",
    "    # 3) Call LLM\n",
    "    response = llm(messages)\n",
    "    answer = response.content\n",
    "\n",
    "    # 4) Update memory\n",
    "    memory.add_turn(question, answer)\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1eb3a4",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Test the Pipeline\n",
    "\n",
    "Now you can ask questions about your **company files** (PDF, DOCX, CSV) in `./data`.\n",
    "\n",
    "Try a few:\n",
    "\n",
    "- ‚ÄúWhat is our refund policy?‚Äù  \n",
    "- ‚ÄúSummarize the 2023 Q4 metrics.‚Äù  \n",
    "- ‚ÄúWhat are the responsibilities of the data engineer role?‚Äù  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e570159",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example interactive loop (run and then type questions)\n",
    "# Stop by interrupting the cell.\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        q = input(\"\\nAsk a question (or 'exit'): \").strip()\n",
    "        if not q or q.lower() == \"exit\":\n",
    "            print(\"Goodbye.\")\n",
    "            break\n",
    "        ans = rag_answer(q, k=5)\n",
    "        print(\"\\n--- Answer ---\")\n",
    "        print(ans)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStopped.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df91f05",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### ‚úÖ Summary (Python Hands-On)\n",
    "\n",
    "This notebook gave you a **minimal but real** RAG stack in Python with:\n",
    "\n",
    "- PDF / DOCX / CSV ingestion,  \n",
    "- Chunking with `RecursiveCharacterTextSplitter`,  \n",
    "- Multiple embeddings (OpenAI + SentenceTransformers),  \n",
    "- Multiple vector stores (FAISS + Chroma),  \n",
    "- Switchable chat models,  \n",
    "- Simple but explicit conversation memory,  \n",
    "- A clean `rag_answer()` function you can integrate into an API later.\n",
    "\n",
    "Use this as your **reference template** and customize per project/domain.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
