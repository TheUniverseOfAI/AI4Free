{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8febb20d",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§ª 04 â€” RAG Hands-On (Python, Option A)\n",
    "\n",
    "Welcome to the **hands-on lab** for building a full **Retrieval-Augmented Generation (RAG)** pipeline in Python.\n",
    "\n",
    "In this notebook you will:\n",
    "\n",
    "- Load **real sample documents** (finance, health, legal, code)\n",
    "- Chunk them using several strategies\n",
    "- Embed them with different embedding backends (OpenAI + local)\n",
    "- Store vectors in a simple local vector store (Chroma or FAISS)\n",
    "- Retrieve and re-rank chunks\n",
    "- Generate answers with citations\n",
    "- Add basic conversational memory\n",
    "- Add simple evaluation hooks (LLM-as-judge style)\n",
    "\n",
    "> âš ï¸ You can run this notebook â€œlocally-firstâ€: all sample docs are contained in this repo under `data/sample_docs/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1e403c",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Environment Setup\n",
    "\n",
    "Uncomment and run the following cells in your own environment.\n",
    "\n",
    "```bash\n",
    "# Create and activate a virtual env (optional)\n",
    "python -m venv .venv\n",
    "source .venv/bin/activate  # on Windows: .venv\\Scripts\\activate\n",
    "\n",
    "# Install core dependencies\n",
    "pip install \"langchain>=0.3.0\" \"langchain-community\" \"langchain-openai\"             \"chromadb\" \"sentence-transformers\" \"pypdf\" \"python-dotenv\"\n",
    "```\n",
    "\n",
    "Create a `.env` file in the repo root:\n",
    "\n",
    "```bash\n",
    "OPENAI_API_KEY=sk-...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abda505a",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Inspect Sample Documents\n",
    "\n",
    "We included a few small sample documents under:\n",
    "\n",
    "- `data/sample_docs/finance_intro.md`\n",
    "- `data/sample_docs/health_intro.md`\n",
    "- `data/sample_docs/legal_clause.md`\n",
    "- `data/sample_docs/code_sample.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e153ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path(\"..\").resolve() if (Path(\".\") / \"notebooks\").exists() else Path(\".\").resolve()\n",
    "samples_dir = base_dir / \"data\" / \"sample_docs\"\n",
    "\n",
    "list(samples_dir.glob(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff4a8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in samples_dir.glob(\"*\"):\n",
    "    print(f\"--- {path.name} ---\")\n",
    "    print(path.read_text()[:300], \"...\n",
    "\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f6fd4b",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Loading Documents\n",
    "\n",
    "We'll keep loading logic simple and transparent:\n",
    "\n",
    "- read `.md` and `.py` files directly\n",
    "- you can later plug in PDF loaders such as `PyPDFLoader` or `PyMuPDFLoader`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805f1859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class RawDocument:\n",
    "    content: str\n",
    "    metadata: dict\n",
    "\n",
    "def load_markdown_files(folder: Path) -> List[RawDocument]:\n",
    "    docs: List[RawDocument] = []\n",
    "    for path in folder.glob(\"*.md\"):\n",
    "        text = path.read_text(encoding=\"utf-8\")\n",
    "        docs.append(RawDocument(content=text, metadata={\"source\": str(path)}))\n",
    "    return docs\n",
    "\n",
    "def load_code_files(folder: Path) -> List[RawDocument]:\n",
    "    docs: List[RawDocument] = []\n",
    "    for path in folder.glob(\"*.py\"):\n",
    "        text = path.read_text(encoding=\"utf-8\")\n",
    "        docs.append(RawDocument(content=text, metadata={\"source\": str(path)}))\n",
    "    return docs\n",
    "\n",
    "raw_docs = load_markdown_files(samples_dir) + load_code_files(samples_dir)\n",
    "len(raw_docs), raw_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a4703f",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Chunking Strategies\n",
    "\n",
    "Weâ€™ll use LangChain's `RecursiveCharacterTextSplitter` for a robust baseline,\n",
    "and show how to adjust chunk size and overlap.\n",
    "\n",
    "Later, you can experiment with token-based or sentence-based splitters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c92c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import Dict\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    content: str\n",
    "    metadata: Dict\n",
    "\n",
    "def chunk_documents(\n",
    "    docs: List[RawDocument],\n",
    "    chunk_size: int = 500,\n",
    "    chunk_overlap: int = 100,\n",
    ") -> List[Chunk]:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    )\n",
    "    chunks: List[Chunk] = []\n",
    "    for d in docs:\n",
    "        for c in splitter.split_text(d.content):\n",
    "            chunks.append(Chunk(content=c, metadata=d.metadata.copy()))\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_documents(raw_docs, chunk_size=400, chunk_overlap=80)\n",
    "len(chunks), chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f97463a",
   "metadata": {},
   "source": [
    "\n",
    "> âœ… At this point you have a set of **chunks** with text and metadata (`source`).\n",
    "\n",
    "You can inspect a few examples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08118606",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ch in enumerate(chunks[:5]):\n",
    "    print(f\"Chunk {i} from {ch.metadata['source']}:\")\n",
    "    print(ch.content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71f6f75",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Embeddings (OpenAI + Local)\n",
    "\n",
    "We'll define a simple embedding interface and support multiple backends:\n",
    "\n",
    "- OpenAI embeddings (e.g., `text-embedding-3-large`)\n",
    "- Local `sentence-transformers` model (offline-friendly)\n",
    "\n",
    "> âš ï¸ Only run OpenAI code if you have an API key configured.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afd0232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Optional\n",
    "import os\n",
    "\n",
    "EmbedModel = Literal[\"openai\", \"local\"]\n",
    "\n",
    "class Embedder:\n",
    "    def __init__(self, model: EmbedModel = \"openai\"):\n",
    "        self.model_type = model\n",
    "        self._local_model = None\n",
    "        self._openai_client = None\n",
    "\n",
    "    def _ensure_local_model(self):\n",
    "        if self._local_model is None:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            self._local_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    def _ensure_openai_client(self):\n",
    "        if self._openai_client is None:\n",
    "            from openai import OpenAI\n",
    "            self._openai_client = OpenAI()\n",
    "\n",
    "    def embed_texts(self, texts: list[str]) -> list[list[float]]:\n",
    "        if self.model_type == \"local\":\n",
    "            self._ensure_local_model()\n",
    "            return self._local_model.encode(texts, convert_to_numpy=False).tolist()\n",
    "        elif self.model_type == \"openai\":\n",
    "            self._ensure_openai_client()\n",
    "            resp = self._openai_client.embeddings.create(\n",
    "                model=\"text-embedding-3-large\",\n",
    "                input=texts,\n",
    "            )\n",
    "            return [d.embedding for d in resp.data]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_type={self.model_type!r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d85948",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1 Embed Our Chunks (Local Model by Default)\n",
    "\n",
    "We'll default to the local model so you can run without paying for tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74db2dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Embedder(model=\"local\")  # change to \"openai\" if you prefer and have key\n",
    "texts = [c.content for c in chunks]\n",
    "embeddings = embedder.embed_texts(texts)\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febd4517",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Simple Vector Store (Chroma)\n",
    "\n",
    "For a hands-on lab, **Chroma** is a great local vector store.\n",
    "\n",
    "We'll build a small wrapper around it to keep the interface simple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb53437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "class ChromaVectorStore:\n",
    "    def __init__(self, persist_directory: Optional[str] = None):\n",
    "        self.persist_directory = persist_directory\n",
    "        self._store = None\n",
    "\n",
    "    def build(self, chunks: list[Chunk]):\n",
    "        from langchain_openai import OpenAIEmbeddings\n",
    "        lc_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "        docs = [\n",
    "            Document(page_content=c.content, metadata=c.metadata)\n",
    "            for c in chunks\n",
    "        ]\n",
    "\n",
    "        self._store = Chroma.from_documents(\n",
    "            documents=docs,\n",
    "            embedding=lc_embeddings,\n",
    "            persist_directory=self.persist_directory,\n",
    "            collection_name=\"rag_demo\",\n",
    "        )\n",
    "\n",
    "    def similarity_search(self, query: str, k: int = 5) -> list[Document]:\n",
    "        return self._store.similarity_search(query, k=k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e5d659",
   "metadata": {},
   "source": [
    "\n",
    "### 5.1 Build the Vector Store\n",
    "\n",
    "This will create (or reuse) a local Chroma directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf035727",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = ChromaVectorStore(persist_directory=\"chroma_rag_demo\")\n",
    "vs.build(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b1cb9",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Retrieval + Answer Generation\n",
    "\n",
    "We can now define a simple **RAG chain**:\n",
    "\n",
    "1. Retrieve top-k documents from Chroma  \n",
    "2. Compose a prompt with:\n",
    "   - user question\n",
    "   - retrieved context  \n",
    "3. Call an LLM (OpenAI or local wrapper) to generate an answer with citations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf56727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def build_context(docs, max_chars: int = 2000) -> str:\n",
    "    ctx_parts = []\n",
    "    total = 0\n",
    "    for d in docs:\n",
    "        snippet = d.page_content[:500]\n",
    "        src = d.metadata.get(\"source\", \"unknown\")\n",
    "        block = f\"Source: {src}\\n{snippet}\"\n",
    "        if total + len(block) > max_chars:\n",
    "            break\n",
    "        ctx_parts.append(block)\n",
    "        total += len(block)\n",
    "    return \"\\n\\n---\\n\\n\".join(ctx_parts)\n",
    "\n",
    "def rag_answer(query: str, k: int = 5, model_name: str = \"gpt-4o-mini\") -> str:\n",
    "    docs = vs.similarity_search(query, k=k)\n",
    "    context = build_context(docs)\n",
    "    system_prompt = (\n",
    "        \"You are a careful assistant. Use ONLY the provided context to answer.\\n\"\n",
    "        \"If the context is insufficient, say you are not sure. Always include brief citations.\"\n",
    "    )\n",
    "    llm = ChatOpenAI(model=model_name)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {query}\\n\\nContext:\\n{context}\"},\n",
    "    ]\n",
    "    resp = llm.invoke(messages)\n",
    "    return resp.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f5a42f",
   "metadata": {},
   "source": [
    "\n",
    "### 6.1 Try a Demo Question\n",
    "\n",
    "Once your environment is set up and you have an OpenAI key, you can run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d213f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example (requires OPENAI_API_KEY)\n",
    "# answer = rag_answer(\"Explain dollar-cost averaging in simple terms.\")\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135af462",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Conversational RAG (Simple Memory)\n",
    "\n",
    "We can add a very light-weight **conversation buffer**:\n",
    "\n",
    "- Keep the last few Q&A pairs\n",
    "- Include them in the prompt as extra context (but separate from retrieved docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f47b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ConversationRAG:\n",
    "    def __init__(self, history_limit: int = 5, model_name: str = \"gpt-4o-mini\"):\n",
    "        self.history = deque(maxlen=history_limit)\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def ask(self, query: str, k: int = 5) -> str:\n",
    "        docs = vs.similarity_search(query, k=k)\n",
    "        rag_context = build_context(docs)\n",
    "\n",
    "        history_str = \"\"\n",
    "        if self.history:\n",
    "            history_str = \"\\n\\nConversation history:\\n\"\n",
    "            for i, (q, a) in enumerate(self.history):\n",
    "                history_str += f\"Q{i+1}: {q}\\nA{i+1}: {a}\\n\"\n",
    "\n",
    "        system_prompt = (\n",
    "            \"You are a conversational RAG assistant.\\n\"\n",
    "            \"Use the retrieved context and (secondarily) the conversation history.\\n\"\n",
    "            \"Be honest if you do not know.\"\n",
    "        )\n",
    "        llm = ChatOpenAI(model=self.model_name)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"{history_str}\\n\\nNew question: {query}\\n\\nRetrieved context:\\n{rag_context}\"},\n",
    "        ]\n",
    "        resp = llm.invoke(messages)\n",
    "        answer = resp.content\n",
    "        self.history.append((query, answer))\n",
    "        return answer\n",
    "\n",
    "conv_rag = ConversationRAG()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e48e3e",
   "metadata": {},
   "source": [
    "\n",
    "You could then run, for example:\n",
    "\n",
    "```python\n",
    "# conv_rag.ask(\"Explain dollar-cost averaging.\")\n",
    "# conv_rag.ask(\"How is that different from investing a lump sum?\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3e8c09",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Simple Evaluation Hook (LLM-as-Judge Concept)\n",
    "\n",
    "We can ask an LLM to self-evaluate RAG answers:\n",
    "\n",
    "- Given: question, context, answer\n",
    "- Ask: how faithful and useful is this?\n",
    "\n",
    "> âš ï¸ This is a light, conceptual version of what youâ€™ll see more formally\n",
    "> in `07_RAG_Evaluation_and_Benchmarks.ipynb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbb3172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_answer(question: str, answer: str, context: str, model_name: str = \"gpt-4o-mini\") -> str:\n",
    "    prompt = f\"\"\"You are evaluating a Retrieval-Augmented Generation (RAG) answer.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Evaluate:\n",
    "- Faithfulness to the context (0â€“10)\n",
    "- Helpfulness (0â€“10)\n",
    "- Brief explanation of any issues.\n",
    "\n",
    "Return a short evaluation report.\n",
    "\"\"\"\n",
    "    llm = ChatOpenAI(model=model_name)\n",
    "    resp = llm.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    return resp.content\n",
    "\n",
    "# Example usage (once you have an answer and context):\n",
    "# docs = vs.similarity_search(\"Explain dollar-cost averaging\", k=4)\n",
    "# ctx = build_context(docs)\n",
    "# ans = rag_answer(\"Explain dollar-cost averaging\", k=4)\n",
    "# print(judge_answer(\"Explain dollar-cost averaging\", ans, ctx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5959fa95",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Next Steps\n",
    "\n",
    "From here, you can:\n",
    "\n",
    "- Swap in **FAISS** instead of Chroma\n",
    "- Add **BM25** for hybrid retrieval\n",
    "- Integrate with your **Agents Universe** (Agentic RAG)\n",
    "- Wrap this into a **FastAPI** or **Streamlit** app\n",
    "- Use the evaluation ideas from `07_RAG_Evaluation_and_Benchmarks.ipynb` to compare:\n",
    "  - different chunking strategies\n",
    "  - different embedding models\n",
    "  - different retrieval parameters\n",
    "\n",
    "This notebook is your **entry point** into the Python side of your RAG Universe â€” a bridge between:\n",
    "\n",
    "- the conceptual reports & diagrams, and  \n",
    "- real, executable RAG pipelines.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
