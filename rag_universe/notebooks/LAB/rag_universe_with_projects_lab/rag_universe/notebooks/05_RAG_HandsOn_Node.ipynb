{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0252362e",
   "metadata": {},
   "source": [
    "\n",
    "# üß™ 05 ‚Äî RAG Hands-On (Node.js, Option A)\n",
    "\n",
    "This notebook is the **Node.js twin** of the Python RAG lab.\n",
    "\n",
    "Goal:\n",
    "\n",
    "- Show you how to build a **minimal but real** RAG pipeline in Node.js:\n",
    "  - load local docs (same ones as Python lab)\n",
    "  - chunk text\n",
    "  - embed with OpenAI (and optionally local embeddings)\n",
    "  - store in a vector DB (Chroma)\n",
    "  - retrieve and answer questions with citations\n",
    "  - prepare for Agentic RAG / MCP integration later\n",
    "\n",
    "> You‚Äôll copy the code snippets from this notebook into your Node project files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1765e667",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Node Project Setup\n",
    "\n",
    "In your terminal:\n",
    "\n",
    "```bash\n",
    "mkdir rag-node-lab\n",
    "cd rag-node-lab\n",
    "\n",
    "npm init -y\n",
    "\n",
    "# Core deps\n",
    "npm install openai @langchain/core @langchain/community chromadb dotenv\n",
    "\n",
    "# (Optional) local embeddings, if you want offline models later\n",
    "# npm install @xenova/transformers\n",
    "```\n",
    "\n",
    "Create a `.env` file:\n",
    "\n",
    "```bash\n",
    "OPENAI_API_KEY=sk-...\n",
    "```\n",
    "\n",
    "And add this to `package.json` to enable ES modules if needed:\n",
    "\n",
    "```jsonc\n",
    "{\n",
    "  \"type\": \"module\",\n",
    "  \"scripts\": {\n",
    "    \"start\": \"node src/index.js\"\n",
    "  }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc570d89",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Sample Documents (Shared with Python Lab)\n",
    "\n",
    "We will reuse the same sample documents from the Python lab:\n",
    "\n",
    "- `data/sample_docs/finance_intro.md`\n",
    "- `data/sample_docs/health_intro.md`\n",
    "- `data/sample_docs/legal_clause.md`\n",
    "- `data/sample_docs/code_sample.py`\n",
    "\n",
    "You can point your Node scripts at the same folder if your repo layout is:\n",
    "\n",
    "```text\n",
    "rag_universe/\n",
    "  data/\n",
    "    sample_docs/\n",
    "      finance_intro.md\n",
    "      health_intro.md\n",
    "      legal_clause.md\n",
    "      code_sample.py\n",
    "  notebooks/\n",
    "    04_RAG_HandsOn_Python.ipynb\n",
    "    05_RAG_HandsOn_Node.ipynb\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124d248f",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Project Structure (Node)\n",
    "\n",
    "Suggested structure for this lab:\n",
    "\n",
    "```text\n",
    "rag-node-lab/\n",
    "  src/\n",
    "    config.js\n",
    "    loaders.js\n",
    "    chunker.js\n",
    "    embeddings.js\n",
    "    vectorStore.js\n",
    "    rag.js\n",
    "    index.js\n",
    "  .env\n",
    "  package.json\n",
    "```\n",
    "\n",
    "You can merge this structure into your bigger **RAG Universe** repo later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5886b6",
   "metadata": {},
   "source": [
    "\n",
    "## 3. `src/config.js` ‚Äî Configuration Helper\n",
    "\n",
    "```js\n",
    "// src/config.js\n",
    "import 'dotenv/config';\n",
    "\n",
    "export const OPENAI_API_KEY = process.env.OPENAI_API_KEY;\n",
    "\n",
    "if (!OPENAI_API_KEY) {\n",
    "  console.warn('‚ö†Ô∏è OPENAI_API_KEY is not set. Some features will not work.');\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a04af5",
   "metadata": {},
   "source": [
    "\n",
    "## 4. `src/loaders.js` ‚Äî Load Local Files\n",
    "\n",
    "We‚Äôll use Node's `fs` and `path` to read `.md` and `.py` files.\n",
    "\n",
    "```js\n",
    "// src/loaders.js\n",
    "import fs from 'fs';\n",
    "import path from 'path';\n",
    "import { fileURLToPath } from 'url';\n",
    "\n",
    "const __filename = fileURLToPath(import.meta.url);\n",
    "const __dirname = path.dirname(__filename);\n",
    "\n",
    "export function loadSampleDocs(relativeDir = '../data/sample_docs') {\n",
    "  const folder = path.resolve(__dirname, relativeDir);\n",
    "  const entries = fs.readdirSync(folder, { withFileTypes: true });\n",
    "\n",
    "  const docs = [];\n",
    "\n",
    "  for (const entry of entries) {\n",
    "    if (!entry.isFile()) continue;\n",
    "    const ext = path.extname(entry.name);\n",
    "    if (!['.md', '.py', '.txt'].includes(ext)) continue;\n",
    "\n",
    "    const fullPath = path.join(folder, entry.name);\n",
    "    const content = fs.readFileSync(fullPath, 'utf-8');\n",
    "    docs.push({\n",
    "      content,\n",
    "      metadata: { source: fullPath },\n",
    "    });\n",
    "  }\n",
    "\n",
    "  return docs;\n",
    "}\n",
    "```\n",
    "\n",
    "You can test this via a small script:\n",
    "\n",
    "```js\n",
    "// src/debugLoad.js\n",
    "import { loadSampleDocs } from './loaders.js';\n",
    "\n",
    "console.log(loadSampleDocs());\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acac57fb",
   "metadata": {},
   "source": [
    "\n",
    "## 5. `src/chunker.js` ‚Äî Simple Chunking\n",
    "\n",
    "We‚Äôll implement a simple character-based chunker.\n",
    "\n",
    "```js\n",
    "// src/chunker.js\n",
    "\n",
    "export function chunkDocuments(docs, chunkSize = 500, overlap = 100) {\n",
    "  const chunks = [];\n",
    "\n",
    "  for (const doc of docs) {\n",
    "    const text = doc.content;\n",
    "    let start = 0;\n",
    "\n",
    "    while (start < text.length) {\n",
    "      const end = Math.min(start + chunkSize, text.length);\n",
    "      const chunkText = text.slice(start, end);\n",
    "      chunks.push({\n",
    "        content: chunkText,\n",
    "        metadata: { ...doc.metadata },\n",
    "      });\n",
    "      start = end - overlap;\n",
    "      if (start < 0) start = 0;\n",
    "      if (start >= text.length) break;\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return chunks;\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ea408b",
   "metadata": {},
   "source": [
    "\n",
    "## 6. `src/embeddings.js` ‚Äî Embedding Wrapper\n",
    "\n",
    "We‚Äôll start with OpenAI embeddings via `openai` and later you can plug in local models.\n",
    "\n",
    "```js\n",
    "// src/embeddings.js\n",
    "import OpenAI from 'openai';\n",
    "import { OPENAI_API_KEY } from './config.js';\n",
    "\n",
    "export class Embedder {\n",
    "  constructor(model = 'openai') {\n",
    "    this.modelType = model;\n",
    "    this.client = null;\n",
    "  }\n",
    "\n",
    "  ensureClient() {\n",
    "    if (!this.client) {\n",
    "      if (!OPENAI_API_KEY) {\n",
    "        throw new Error('OPENAI_API_KEY is not set');\n",
    "      }\n",
    "      this.client = new OpenAI({ apiKey: OPENAI_API_KEY });\n",
    "    }\n",
    "  }\n",
    "\n",
    "  async embedTexts(texts) {\n",
    "    if (this.modelType === 'openai') {\n",
    "      this.ensureClient();\n",
    "      const response = await this.client.embeddings.create({\n",
    "        model: 'text-embedding-3-large',\n",
    "        input: texts,\n",
    "      });\n",
    "      return response.data.map(d => d.embedding);\n",
    "    }\n",
    "\n",
    "    // Placeholder for local embeddings:\n",
    "    // if (this.modelType === 'local') { ... }\n",
    "\n",
    "    throw new Error(`Unknown embedding model type: ${this.modelType}`);\n",
    "  }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbd112e",
   "metadata": {},
   "source": [
    "\n",
    "## 7. `src/vectorStore.js` ‚Äî Chroma Vector Store\n",
    "\n",
    "We‚Äôll use the JS bindings for Chroma.\n",
    "\n",
    "```js\n",
    "// src/vectorStore.js\n",
    "import { ChromaClient } from 'chromadb';\n",
    "\n",
    "export class ChromaVectorStore {\n",
    "  constructor(collectionName = 'rag_demo') {\n",
    "    this.client = new ChromaClient();\n",
    "    this.collectionName = collectionName;\n",
    "    this.collection = null;\n",
    "  }\n",
    "\n",
    "  async init() {\n",
    "    this.collection = await this.client.getOrCreateCollection({\n",
    "      name: this.collectionName,\n",
    "    });\n",
    "  }\n",
    "\n",
    "  async addDocuments(chunks, embeddings) {\n",
    "    if (!this.collection) {\n",
    "      await this.init();\n",
    "    }\n",
    "    const ids = chunks.map((_, i) => `chunk-${i}`);\n",
    "    const texts = chunks.map(c => c.content);\n",
    "    const metadatas = chunks.map(c => c.metadata);\n",
    "\n",
    "    await this.collection.add({\n",
    "      ids,\n",
    "      documents: texts,\n",
    "      metadatas,\n",
    "      embeddings,\n",
    "    });\n",
    "  }\n",
    "\n",
    "  async similaritySearch(queryEmbedding, k = 5) {\n",
    "    if (!this.collection) {\n",
    "      await this.init();\n",
    "    }\n",
    "    const result = await this.collection.query({\n",
    "      queryEmbeddings: [queryEmbedding],\n",
    "      nResults: k,\n",
    "    });\n",
    "\n",
    "    const docs = [];\n",
    "    const { documents, metadatas } = result;\n",
    "\n",
    "    if (documents && documents[0]) {\n",
    "      for (let i = 0; i < documents[0].length; i++) {\n",
    "        docs.push({\n",
    "          pageContent: documents[0][i],\n",
    "          metadata: metadatas ? metadatas[0][i] : {},\n",
    "        });\n",
    "      }\n",
    "    }\n",
    "\n",
    "    return docs;\n",
    "  }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6440b16e",
   "metadata": {},
   "source": [
    "\n",
    "## 8. `src/rag.js` ‚Äî Retrieval + Answer\n",
    "\n",
    "Now we glue everything together:\n",
    "\n",
    "```js\n",
    "// src/rag.js\n",
    "import OpenAI from 'openai';\n",
    "import { loadSampleDocs } from './loaders.js';\n",
    "import { chunkDocuments } from './chunker.js';\n",
    "import { Embedder } from './embeddings.js';\n",
    "import { ChromaVectorStore } from './vectorStore.js';\n",
    "import { OPENAI_API_KEY } from './config.js';\n",
    "\n",
    "const openaiClient = new OpenAI({ apiKey: OPENAI_API_KEY });\n",
    "\n",
    "export async function buildRagIndex() {\n",
    "  const docs = loadSampleDocs(); // local sample docs\n",
    "  const chunks = chunkDocuments(docs, 400, 80);\n",
    "\n",
    "  const embedder = new Embedder('openai');\n",
    "  const texts = chunks.map(c => c.content);\n",
    "  const embeddings = await embedder.embedTexts(texts);\n",
    "\n",
    "  const store = new ChromaVectorStore('rag_demo_node');\n",
    "  await store.addDocuments(chunks, embeddings);\n",
    "\n",
    "  return store;\n",
    "}\n",
    "\n",
    "function buildContext(docs, maxChars = 2000) {\n",
    "  let total = 0;\n",
    "  const parts = [];\n",
    "\n",
    "  for (const d of docs) {\n",
    "    const snippet = d.pageContent.slice(0, 500);\n",
    "    const src = d.metadata?.source ?? 'unknown';\n",
    "    const block = `Source: ${src}\\n${snippet}`;\n",
    "    if (total + block.length > maxChars) break;\n",
    "    parts.push(block);\n",
    "    total += block.length;\n",
    "  }\n",
    "\n",
    "  return parts.join('\\n\\n---\\n\\n');\n",
    "}\n",
    "\n",
    "export async function ragAnswer(store, query, k = 5, model = 'gpt-4o-mini') {\n",
    "  // 1) Embed query\n",
    "  const embedder = new Embedder('openai');\n",
    "  const [queryEmbedding] = await embedder.embedTexts([query]);\n",
    "\n",
    "  // 2) Retrieve\n",
    "  const docs = await store.similaritySearch(queryEmbedding, k);\n",
    "  const context = buildContext(docs);\n",
    "\n",
    "  // 3) Generate\n",
    "  const systemPrompt = [\n",
    "    'You are a careful RAG assistant.',\n",
    "    'Use ONLY the provided context to answer.',\n",
    "    'If the context is insufficient, say you are not sure.',\n",
    "    'Always include brief citations mentioning the source paths.',\n",
    "  ].join(' ');\n",
    "\n",
    "  const completion = await openaiClient.chat.completions.create({\n",
    "    model,\n",
    "    messages: [\n",
    "      { role: 'system', content: systemPrompt },\n",
    "      {\n",
    "        role: 'user',\n",
    "        content: `Question: ${query}\\n\\nContext:\\n${context}`,\n",
    "      },\n",
    "    ],\n",
    "  });\n",
    "\n",
    "  return completion.choices[0]?.message?.content ?? '';\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ee578e",
   "metadata": {},
   "source": [
    "\n",
    "## 9. `src/index.js` ‚Äî Run a Demo Query\n",
    "\n",
    "```js\n",
    "// src/index.js\n",
    "import { buildRagIndex, ragAnswer } from './rag.js';\n",
    "\n",
    "async function main() {\n",
    "  const store = await buildRagIndex();\n",
    "  const query = 'Explain dollar-cost averaging in simple terms.';\n",
    "  const answer = await ragAnswer(store, query, 5);\n",
    "  console.log('Q:', query);\n",
    "  console.log('\\nA:', answer);\n",
    "}\n",
    "\n",
    "main().catch(err => {\n",
    "  console.error(err);\n",
    "  process.exit(1);\n",
    "});\n",
    "```\n",
    "\n",
    "Run:\n",
    "\n",
    "```bash\n",
    "node src/index.js\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f9d805",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Next Steps (Node.js)\n",
    "\n",
    "From here, you can:\n",
    "\n",
    "- Swap Chroma for **Pinecone** using the JS client\n",
    "- Add a **reranker** (e.g., Cohere or LLM-based)\n",
    "- Implement a **conversational wrapper** that keeps short-term history\n",
    "- Integrate with your **Agents Universe** (Agentic RAG in Node)\n",
    "- Expose as an API using **Express**, **Fastify**, or **Hono**\n",
    "- Build a simple **React / Next.js** UI that calls this RAG backend\n",
    "\n",
    "This notebook plus the Python one give you:\n",
    "\n",
    "- A **cross-language mental model**\n",
    "- A clear, runnable starting point for Node-based RAG pipelines\n",
    "- A direct bridge into your **RAG + Agents + MCP** architecture.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
