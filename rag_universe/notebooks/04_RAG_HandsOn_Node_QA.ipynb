{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f41ebe",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸŸ¦ RAG Hands-On (Node.js) â€” Company Files Q&A Chatbot (Backend Only)\n",
    "\n",
    "**Goal:** Sketch a **clean Node.js RAG backend** for Q&A over company files (PDF, DOCX, CSV):\n",
    "\n",
    "- No UI, no Tavily, no web search â€” just a backend-style pipeline.  \n",
    "- Uses **popular models & tools** via LangChain.js-style patterns.  \n",
    "- Offline-friendly vector stores (FAISS / Memory / Chroma client).  \n",
    "- Clear structure so you can turn this into a real Node project.\n",
    "\n",
    "> This notebook is mostly **Markdown + code snippets**.  \n",
    "> Copy the code into your Node project (`src/` folder) and run there.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e4ffc9",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Project Structure (suggested)\n",
    "\n",
    "```bash\n",
    "rag-node-backend/\n",
    "  package.json\n",
    "  tsconfig.json           # if using TypeScript\n",
    "  src/\n",
    "    config.ts\n",
    "    loaders/\n",
    "      pdfLoader.ts\n",
    "      docxLoader.ts\n",
    "      csvLoader.ts\n",
    "    chunking.ts\n",
    "    embeddings.ts\n",
    "    vectorstores.ts\n",
    "    llms.ts\n",
    "    memory.ts\n",
    "    rag.ts\n",
    "    server.ts             # optional HTTP server for testing\n",
    "  data/\n",
    "    ... your PDF/DOCX/CSV files ...\n",
    "```\n",
    "\n",
    "You can keep it all in one file at the beginning (e.g. `rag.ts`),  \n",
    "then refactor into modules as it grows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db514d9a",
   "metadata": {},
   "source": [
    "\n",
    "## 1. `package.json` (Core Dependencies)\n",
    "\n",
    "Example for **TypeScript + Node**:\n",
    "\n",
    "```jsonc\n",
    "{\n",
    "  \"name\": \"rag-node-backend\",\n",
    "  \"version\": \"1.0.0\",\n",
    "  \"type\": \"module\",\n",
    "  \"main\": \"dist/server.js\",\n",
    "  \"scripts\": {\n",
    "    \"dev\": \"ts-node src/server.ts\",\n",
    "    \"build\": \"tsc\",\n",
    "    \"start\": \"node dist/server.js\"\n",
    "  },\n",
    "  \"dependencies\": {\n",
    "    \"langchain\": \"^0.3.0\",\n",
    "    \"@langchain/community\": \"^0.3.0\",\n",
    "    \"@langchain/openai\": \"^0.3.0\",\n",
    "    \"faiss-node\": \"^0.5.0\",\n",
    "    \"chromadb\": \"^1.8.0\",\n",
    "    \"pdf-parse\": \"^1.1.1\",\n",
    "    \"docx\": \"^9.0.0\",\n",
    "    \"csv-parse\": \"^5.5.0\",\n",
    "    \"dotenv\": \"^16.0.0\"\n",
    "  },\n",
    "  \"devDependencies\": {\n",
    "    \"typescript\": \"^5.0.0\",\n",
    "    \"ts-node\": \"^10.0.0\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "> Versions are illustrative â€” adjust to current ones when you actually install.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ddabad",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Config & Environment (`src/config.ts`)\n",
    "\n",
    "```ts\n",
    "// src/config.ts\n",
    "import * as dotenv from \"dotenv\";\n",
    "dotenv.config();\n",
    "\n",
    "export const DATA_DIR = \"./data\";\n",
    "\n",
    "export const OPENAI_API_KEY = process.env.OPENAI_API_KEY || \"\";\n",
    "\n",
    "// Choose default models\n",
    "export const DEFAULT_CHAT_MODEL = \"gpt-4o-mini\"; // or your available model\n",
    "export const DEFAULT_EMBEDDING_MODEL = \"text-embedding-3-small\";\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54407611",
   "metadata": {},
   "source": [
    "\n",
    "## 3. File Loaders (`src/loaders/*`)\n",
    "\n",
    "Simplified approach using common Node libs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e421fb",
   "metadata": {},
   "source": [
    "\n",
    "### `src/loaders/pdfLoader.ts`\n",
    "\n",
    "```ts\n",
    "import fs from \"fs\";\n",
    "import path from \"path\";\n",
    "import pdfParse from \"pdf-parse\";\n",
    "\n",
    "export async function loadPdf(filePath: string) {\n",
    "  const dataBuffer = fs.readFileSync(filePath);\n",
    "  const pdfData = await pdfParse(dataBuffer);\n",
    "  // Simple: one big doc; you can also split by page if needed\n",
    "  return [\n",
    "    {\n",
    "      pageContent: pdfData.text,\n",
    "      metadata: { source: filePath },\n",
    "    },\n",
    "  ];\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d82599a",
   "metadata": {},
   "source": [
    "\n",
    "### `src/loaders/docxLoader.ts`\n",
    "\n",
    "```ts\n",
    "import fs from \"fs\";\n",
    "import { Document, Packer } from \"docx\"; // docx parsing is a bit tricky; pseudo-code here\n",
    "\n",
    "export async function loadDocx(filePath: string) {\n",
    "  // In practice, consider using a higher-level docx-to-text library\n",
    "  const buffer = fs.readFileSync(filePath);\n",
    "  // Placeholder: you'd replace this with actual DOCX text extraction\n",
    "  const text = buffer.toString(\"utf8\");\n",
    "\n",
    "  return [\n",
    "    {\n",
    "      pageContent: text,\n",
    "      metadata: { source: filePath },\n",
    "    },\n",
    "  ];\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fee0c1",
   "metadata": {},
   "source": [
    "\n",
    "### `src/loaders/csvLoader.ts`\n",
    "\n",
    "```ts\n",
    "import fs from \"fs\";\n",
    "import { parse } from \"csv-parse/sync\";\n",
    "\n",
    "export async function loadCsv(filePath: string, textCols?: string[]) {\n",
    "  const content = fs.readFileSync(filePath, \"utf8\");\n",
    "  const records = parse(content, { columns: true, skip_empty_lines: true });\n",
    "\n",
    "  const docs = records.map((row: any, idx: number) => {\n",
    "    const cols = textCols || Object.keys(row);\n",
    "    const pieces = cols\n",
    "      .filter((col) => row[col] !== undefined && row[col] !== null)\n",
    "      .map((col) => `${col}: ${row[col]}`);\n",
    "    const text = pieces.join(\"\\n\");\n",
    "    return {\n",
    "      pageContent: text,\n",
    "      metadata: { source: filePath, row: idx },\n",
    "    };\n",
    "  });\n",
    "\n",
    "  return docs;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17b2e25",
   "metadata": {},
   "source": [
    "\n",
    "### Aggregate loader (`src/loaders/index.ts`)\n",
    "\n",
    "```ts\n",
    "// src/loaders/index.ts\n",
    "import fs from \"fs\";\n",
    "import path from \"path\";\n",
    "import { loadPdf } from \"./pdfLoader\";\n",
    "import { loadDocx } from \"./docxLoader\";\n",
    "import { loadCsv } from \"./csvLoader\";\n",
    "\n",
    "export interface LCSDocument {\n",
    "  pageContent: string;\n",
    "  metadata: Record<string, any>;\n",
    "}\n",
    "\n",
    "export async function loadAllDocuments(dataDir: string): Promise<LCSDocument[]> {\n",
    "  const docs: LCSDocument[] = [];\n",
    "\n",
    "  function walk(dir: string) {\n",
    "    const entries = fs.readdirSync(dir, { withFileTypes: true });\n",
    "    for (const entry of entries) {\n",
    "      const full = path.join(dir, entry.name);\n",
    "      if (entry.isDirectory()) walk(full);\n",
    "      else {\n",
    "        if (entry.name.toLowerCase().endsWith(\".pdf\")) {\n",
    "          // PDF\n",
    "        } else if (entry.name.toLowerCase().endsWith(\".docx\")) {\n",
    "          // DOCX\n",
    "        } else if (entry.name.toLowerCase().endsWith(\".csv\")) {\n",
    "          // CSV\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  const entries = fs.readdirSync(dataDir, { withFileTypes: true });\n",
    "  for (const entry of entries) {\n",
    "    const full = path.join(dataDir, entry.name);\n",
    "    if (entry.isDirectory()) {\n",
    "      docs.push(...(await loadAllDocuments(full)));\n",
    "    } else if (entry.name.toLowerCase().endsWith(\".pdf\")) {\n",
    "      docs.push(...(await loadPdf(full)));\n",
    "    } else if (entry.name.toLowerCase().endsWith(\".docx\")) {\n",
    "      docs.push(...(await loadDocx(full)));\n",
    "    } else if (entry.name.toLowerCase().endsWith(\".csv\")) {\n",
    "      docs.push(...(await loadCsv(full)));\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return docs;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e8e879",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Chunking (`src/chunking.ts`)\n",
    "\n",
    "LangChain.js provides text splitters similar to Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd560bc",
   "metadata": {},
   "source": [
    "\n",
    "```ts\n",
    "// src/chunking.ts\n",
    "import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n",
    "import type { LCSDocument } from \"./loaders\";\n",
    "\n",
    "export async function chunkDocuments(\n",
    "  docs: LCSDocument[],\n",
    "  chunkSize = 1000,\n",
    "  chunkOverlap = 200\n",
    "): Promise<LCSDocument[]> {\n",
    "  const splitter = new RecursiveCharacterTextSplitter({\n",
    "    chunkSize,\n",
    "    chunkOverlap,\n",
    "    separators: [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "  });\n",
    "\n",
    "  const allChunks: LCSDocument[] = [];\n",
    "  for (const d of docs) {\n",
    "    const split = await splitter.splitText(d.pageContent);\n",
    "    split.forEach((text) => {\n",
    "      allChunks.push({\n",
    "        pageContent: text,\n",
    "        metadata: d.metadata,\n",
    "      });\n",
    "    });\n",
    "  }\n",
    "\n",
    "  return allChunks;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa741a",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Embeddings & Vector Stores (`src/embeddings.ts`, `src/vectorstores.ts`)\n",
    "\n",
    "We define multiple embedding models + vector stores.\n",
    "\n",
    "- Embeddings:\n",
    "  - OpenAI embeddings\n",
    "  - Local model (e.g., `@langchain/community/embeddings/hf`) â€” placeholder here\n",
    "- Vector stores:\n",
    "  - FAISS (via `faiss-node` or LangChain bindings)\n",
    "  - Memory store for quick testing\n",
    "  - Chroma (requires running a Chroma server)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345fe812",
   "metadata": {},
   "source": [
    "\n",
    "```ts\n",
    "// src/embeddings.ts\n",
    "import { OpenAIEmbeddings } from \"@langchain/openai\";\n",
    "// You can also import HF embeddings if desired:\n",
    "// import { HuggingFaceInferenceEmbeddings } from \"@langchain/community/embeddings/hf\";\n",
    "\n",
    "export type EmbeddingName = \"openai_small\" | \"openai_large\";\n",
    "\n",
    "export function getEmbeddingModel(name: EmbeddingName) {\n",
    "  if (name === \"openai_small\") {\n",
    "    return new OpenAIEmbeddings({\n",
    "      model: \"text-embedding-3-small\",\n",
    "    });\n",
    "  }\n",
    "  if (name === \"openai_large\") {\n",
    "    return new OpenAIEmbeddings({\n",
    "      model: \"text-embedding-3-large\",\n",
    "    });\n",
    "  }\n",
    "  throw new Error(`Unknown embedding model: ${name}`);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e915e07c",
   "metadata": {},
   "source": [
    "\n",
    "```ts\n",
    "// src/vectorstores.ts\n",
    "import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n",
    "import { FaissStore } from \"@langchain/community/vectorstores/faiss\";\n",
    "import { ChromaClient } from \"chromadb\";\n",
    "import type { EmbeddingsInterface } from \"@langchain/core/embeddings\";\n",
    "import type { LCSDocument } from \"./loaders\";\n",
    "\n",
    "export type VectorStoreName = \"memory\" | \"faiss\" | \"chroma\";\n",
    "\n",
    "export async function buildVectorStore(\n",
    "  chunks: LCSDocument[],\n",
    "  storeName: VectorStoreName,\n",
    "  embeddings: EmbeddingsInterface\n",
    ") {\n",
    "  if (storeName === \"memory\") {\n",
    "    return await MemoryVectorStore.fromTexts(\n",
    "      chunks.map((c) => c.pageContent),\n",
    "      chunks.map((c) => c.metadata),\n",
    "      embeddings\n",
    "    );\n",
    "  }\n",
    "\n",
    "  if (storeName === \"faiss\") {\n",
    "    return await FaissStore.fromTexts(\n",
    "      chunks.map((c) => c.pageContent),\n",
    "      chunks.map((c) => c.metadata),\n",
    "      embeddings\n",
    "    );\n",
    "  }\n",
    "\n",
    "  if (storeName === \"chroma\") {\n",
    "    const client = new ChromaClient({ path: \"http://localhost:8000\" });\n",
    "    const collectionName = \"company_docs\";\n",
    "    const collection = await client.getOrCreateCollection({ name: collectionName });\n",
    "    // You would then upsert docs into Chroma here (ids, embeddings, metadata).\n",
    "    // For brevity, we skip full implementation.\n",
    "    return collection;\n",
    "  }\n",
    "\n",
    "  throw new Error(`Unknown vector store: ${storeName}`);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1664121",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Chat Models & Memory (`src/llms.ts`, `src/memory.ts`)\n",
    "\n",
    "Use several chat models (by name) and a simple in-memory conversation buffer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf849fe5",
   "metadata": {},
   "source": [
    "\n",
    "```ts\n",
    "// src/llms.ts\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "export type ChatModelName = \"gpt_4_small\" | \"gpt_4_full\";\n",
    "\n",
    "export function getChatModel(name: ChatModelName) {\n",
    "  if (name === \"gpt_4_small\") {\n",
    "    return new ChatOpenAI({\n",
    "      model: \"gpt-4o-mini\",\n",
    "      temperature: 0,\n",
    "    });\n",
    "  }\n",
    "  if (name === \"gpt_4_full\") {\n",
    "    return new ChatOpenAI({\n",
    "      model: \"gpt-4.1\",\n",
    "      temperature: 0,\n",
    "    });\n",
    "  }\n",
    "  throw new Error(`Unknown chat model: ${name}`);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a761f89d",
   "metadata": {},
   "source": [
    "\n",
    "```ts\n",
    "// src/memory.ts\n",
    "export interface ConversationTurn {\n",
    "  user: string;\n",
    "  assistant: string;\n",
    "}\n",
    "\n",
    "export class SimpleConversationMemory {\n",
    "  private maxTurns: number;\n",
    "  private history: ConversationTurn[];\n",
    "\n",
    "  constructor(maxTurns = 5) {\n",
    "    this.maxTurns = maxTurns;\n",
    "    this.history = [];\n",
    "  }\n",
    "\n",
    "  addTurn(user: string, assistant: string) {\n",
    "    this.history.push({ user, assistant });\n",
    "    if (this.history.length > this.maxTurns) {\n",
    "      this.history.shift();\n",
    "    }\n",
    "  }\n",
    "\n",
    "  formatHistory(): string {\n",
    "    return this.history\n",
    "      .map(\n",
    "        (h) =>\n",
    "          `User: ${h.user}\\nAssistant: ${h.assistant}`\n",
    "      )\n",
    "      .join(\"\\n\");\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ff3da5",
   "metadata": {},
   "source": [
    "\n",
    "## 7. RAG Pipeline (`src/rag.ts`)\n",
    "\n",
    "Tie everything together:\n",
    "\n",
    "- Load & chunk docs,\n",
    "- Build vector store,\n",
    "- Retrieve top-k chunks,\n",
    "- Build prompt with context + history,\n",
    "- Call chat model,\n",
    "- Update memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9796b35",
   "metadata": {},
   "source": [
    "\n",
    "```ts\n",
    "// src/rag.ts\n",
    "import { loadAllDocuments } from \"./loaders\";\n",
    "import { chunkDocuments } from \"./chunking\";\n",
    "import { getEmbeddingModel, EmbeddingName } from \"./embeddings\";\n",
    "import { buildVectorStore, VectorStoreName } from \"./vectorstores\";\n",
    "import { getChatModel, ChatModelName } from \"./llms\";\n",
    "import { SimpleConversationMemory } from \"./memory\";\n",
    "import { DATA_DIR } from \"./config\";\n",
    "import { HumanMessage, SystemMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "export class RAGPipeline {\n",
    "  private embeddingName: EmbeddingName;\n",
    "  private vsName: VectorStoreName;\n",
    "  private chatName: ChatModelName;\n",
    "  private memory: SimpleConversationMemory;\n",
    "  private vectorStore: any;\n",
    "  private chatModel: any;\n",
    "\n",
    "  constructor(\n",
    "    embeddingName: EmbeddingName = \"openai_small\",\n",
    "    vsName: VectorStoreName = \"memory\",\n",
    "    chatName: ChatModelName = \"gpt_4_small\"\n",
    "  ) {\n",
    "    this.embeddingName = embeddingName;\n",
    "    this.vsName = vsName;\n",
    "    this.chatName = chatName;\n",
    "    this.memory = new SimpleConversationMemory(5);\n",
    "  }\n",
    "\n",
    "  async init() {\n",
    "    const rawDocs = await loadAllDocuments(DATA_DIR);\n",
    "    const chunks = await chunkDocuments(rawDocs);\n",
    "\n",
    "    const embeddings = getEmbeddingModel(this.embeddingName);\n",
    "    this.vectorStore = await buildVectorStore(chunks, this.vsName, embeddings);\n",
    "    this.chatModel = getChatModel(this.chatName);\n",
    "  }\n",
    "\n",
    "  private buildContextFromDocs(docs: any[]): string {\n",
    "    return docs\n",
    "      .map((d, idx) => {\n",
    "        const src = d.metadata?.source || \"unknown\";\n",
    "        return `[${idx + 1} | ${src}]\\n${d.pageContent}`;\n",
    "      })\n",
    "      .join(\"\\n---\\n\");\n",
    "  }\n",
    "\n",
    "  async answer(question: string, k = 5): Promise<string> {\n",
    "    // 1) Retrieve\n",
    "    const docs = await this.vectorStore.similaritySearch(question, k);\n",
    "    const contextText = this.buildContextFromDocs(docs);\n",
    "\n",
    "    const historyText = this.memory.formatHistory();\n",
    "    const historyBlock = historyText\n",
    "      ? `\\n\\nConversation so far:\\n${historyText}`\n",
    "      : \"\";\n",
    "\n",
    "    const systemPrompt =\n",
    "      \"You are a helpful assistant answering questions based only on the provided context. \" +\n",
    "      \"If the answer is not in the context, say you don't know.\";\n",
    "\n",
    "    const userContent = `Context:\\n${contextText}\\n\\n${historyBlock}\\n\\nUser question: ${question}\\n\\nAnswer:`;\n",
    "\n",
    "    const messages = [\n",
    "      new SystemMessage(systemPrompt),\n",
    "      new HumanMessage(userContent),\n",
    "    ];\n",
    "\n",
    "    const response = await this.chatModel.invoke(messages);\n",
    "    const answer = response.content as string;\n",
    "\n",
    "    this.memory.addTurn(question, answer);\n",
    "    return answer;\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d5e79b",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Simple Test Server (`src/server.ts`)\n",
    "\n",
    "Optional: expose `/ask` endpoint for quick manual testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25ba525",
   "metadata": {},
   "source": [
    "\n",
    "```ts\n",
    "// src/server.ts\n",
    "import http from \"http\";\n",
    "import { RAGPipeline } from \"./rag\";\n",
    "\n",
    "async function main() {\n",
    "  const rag = new RAGPipeline(\"openai_small\", \"memory\", \"gpt_4_small\");\n",
    "  await rag.init();\n",
    "\n",
    "  const server = http.createServer(async (req, res) => {\n",
    "    if (req.method === \"POST\" && req.url === \"/ask\") {\n",
    "      let body = \"\";\n",
    "      req.on(\"data\", (chunk) => (body += chunk.toString()));\n",
    "      req.on(\"end\", async () => {\n",
    "        try {\n",
    "          const { question } = JSON.parse(body);\n",
    "          const answer = await rag.answer(question, 5);\n",
    "          res.writeHead(200, { \"Content-Type\": \"application/json\" });\n",
    "          res.end(JSON.stringify({ answer }));\n",
    "        } catch (err: any) {\n",
    "          res.writeHead(500, { \"Content-Type\": \"application/json\" });\n",
    "          res.end(JSON.stringify({ error: err.message }));\n",
    "        }\n",
    "      });\n",
    "    } else {\n",
    "      res.writeHead(404);\n",
    "      res.end(\"Not found\");\n",
    "    }\n",
    "  });\n",
    "\n",
    "  const port = 3000;\n",
    "  server.listen(port, () => {\n",
    "    console.log(`RAG server listening on http://localhost:${port}`);\n",
    "  });\n",
    "}\n",
    "\n",
    "main().catch((err) => {\n",
    "  console.error(\"Failed to start server:\", err);\n",
    "});\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312ee7ba",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### âœ… Summary (Node.js Hands-On)\n",
    "\n",
    "This notebook gave you a **backend-oriented RAG design in Node.js**:\n",
    "\n",
    "- Loaders for **PDF / DOCX / CSV**,  \n",
    "- Chunking with `RecursiveCharacterTextSplitter`,  \n",
    "- Multiple embeddings (OpenAI; extendable to HF),  \n",
    "- Multiple vector stores (Memory, FAISS, Chroma client),  \n",
    "- Configurable chat models,  \n",
    "- Simple conversation memory,  \n",
    "- A `RAGPipeline` class + optional HTTP server.\n",
    "\n",
    "Use this as your **blueprint** to implement a real Node.js RAG backend for company Q&A.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
